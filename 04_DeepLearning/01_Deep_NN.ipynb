{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파이토치 프레임워크 불러오는 부분\n",
    "import torch\n",
    "import torchvision # image processing에 특화된 라이브러리\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms # Data Augmentation에 특화된 라이브러리\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.Tensor(2,3) # 2행 3열의 텐서를 하나 생성\n",
    "X.shape\n",
    "X.size()\n",
    "\n",
    "X = torch.Tensor([[1,2,3],[4,5,6]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 4.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "tensor()인자값으로\n",
    "data : 값지정 , 초기화\n",
    "dtype\n",
    "requires_grad : tensor에 대해서 미분 적용할지 여부\n",
    "'''\n",
    "x = torch.tensor(data=[2.0, 3.0], requires_grad=True)\n",
    "x\n",
    "\n",
    "y = x**2  # 4, 9\n",
    "y\n",
    "\n",
    "pred = 2*y +3 # 11, 21\n",
    "\n",
    "target = torch.tensor([3.0, 4.0])  # 3, 4\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "위에 나온 결과를 바탕으로\n",
    "경사하강법을 이용해서 \n",
    "미분을 사용해서 Loss에 대한 책임을 묻겠다\n",
    "기울이가 계산이 될 것이다.(미분 계산)\n",
    "--> BackPropagation 을 적용하려면 backward() 함수를 호출\n",
    "\n",
    "Loss를 줄여나가 보겠다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(25., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.sum(torch.abs(pred - target))\n",
    "print('Loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8., 12.]) None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-f0cd9ca8c1fa>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  print(x.grad,pred.grad)\n"
     ]
    }
   ],
   "source": [
    "print(x.grad,pred.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tensor ....Using NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6343, -0.3771, -0.1529],\n",
      "        [ 0.0043,  0.3213,  0.2549],\n",
      "        [-0.6464,  1.7498, -0.3769],\n",
      "        [-0.1006, -1.4268,  0.8419],\n",
      "        [ 1.0721,  1.5363, -2.5409],\n",
      "        [-0.7414, -2.0082,  0.7942],\n",
      "        [-1.2485, -1.0378,  0.1819],\n",
      "        [ 0.5640,  2.0952,  0.2436],\n",
      "        [-0.4538,  0.9258,  0.3400],\n",
      "        [ 0.8474,  0.3718,  0.0628]])\n",
      "tensor([[ 1.6591,  0.4949],\n",
      "        [-0.5507,  0.3667],\n",
      "        [ 1.3947,  0.2634],\n",
      "        [ 1.1020,  0.6948],\n",
      "        [ 1.5271, -0.0417],\n",
      "        [-0.2883,  1.2307],\n",
      "        [ 0.1074,  1.9449],\n",
      "        [ 0.7615,  0.2145],\n",
      "        [-1.5247, -0.0108],\n",
      "        [ 1.0265, -0.1139]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10,3)\n",
    "y = torch.randn(10,2)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w Parameter containing:\n",
      "tensor([[ 0.5442, -0.4392, -0.3906],\n",
      "        [-0.3340,  0.1578,  0.1843]], requires_grad=True)\n",
      "b Parameter containing:\n",
      "tensor([-0.2954, -0.2118], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(3,2)\n",
    "print('w',linear.weight)\n",
    "print('b',linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000002422DBEBDD0>\n",
      "loss before step BackPropagation 1.2180315256118774\n"
     ]
    }
   ],
   "source": [
    "# 객체출력, parameters()함수는 모델안의 학습의 주체인 w, b를 내포하고 있는 객체\n",
    "# parameters() 함수는 w,b를 해킹한 함수이다.\n",
    "print(linear.parameters())\n",
    "\n",
    "# loss function을 미리 선정의 해두었다.\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# optimizer(하산하는 방법)를 미리 정의\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# 위에서 만든 모델에 값을 입력...결과로 예측값이 나온다\n",
    "pred=linear(x)\n",
    "\n",
    "# 정답과 예측값을 이용해서 위에서 선정의한 Loss 를 계산...L(W)\n",
    "loss = loss_function(pred, y)\n",
    "print('loss before step BackPropagation',loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw tensor([[-0.1362, -0.7179,  0.2245],\n",
      "        [ 0.1557,  0.4533, -0.0275]])\n",
      "dL/db tensor([-0.9013, -0.6864])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "loss값이 나왔다는 것은\n",
    "모델의 예측값에 대한 잘잘못을 정량화 했다는 것이다.\n",
    "\n",
    "이 값을 바탕으로 BackPropagation을 하면 w, b에 대한 미분값\n",
    "즉 얼마만큼 보정해야 하는지의 값이 나온다.\n",
    "그 값을 이용해서 다시 하강(기울기 수정)하기 떄문에\n",
    "2번쨰에는 Loss가 줄어들어야 한다.\n",
    "'''\n",
    "loss.backward() #loss값에 대한 w의 책임을 묻는다...미분을 적용. grad\n",
    "print('dL/dw', linear.weight.grad)\n",
    "print('dL/db', linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss afer BackPropagation :  1.197208046913147\n"
     ]
    }
   ],
   "source": [
    "optimizer.step() # 위에서 수정된 값으로 하강을 진행한다...학습을 한다\n",
    "\n",
    "# 반복효과\n",
    "pred = linear(x)\n",
    "loss = loss_function(pred, y)\n",
    "\n",
    "print('loss afer BackPropagation : ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
