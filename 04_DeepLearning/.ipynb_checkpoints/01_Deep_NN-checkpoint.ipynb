{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파이토치 프레임워크 불러오는 부분\n",
    "import torch\n",
    "import torchvision # image processing에 특화된 라이브러리\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms # Data Augmentation 특화된 라이브러리\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.Tensor(2,3) # 2행 3열의 텐서를 하나 생성\n",
    "X.shape\n",
    "X.size()\n",
    "\n",
    "X = torch.Tensor([[1,2,3], [4,5,6]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 4.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "tensor() 인자값으로 \n",
    "data : 값지정, 초기화\n",
    "dtype \n",
    "requires_grad : tensor에 대해서 미분 적용할지 여부\n",
    "'''\n",
    "\n",
    "x = torch.tensor(data=[2.0,3.0],requires_grad=True)\n",
    "x\n",
    "\n",
    "y = x**2 # 4,9\n",
    "y\n",
    "\n",
    "pred = 2*y +3 # 11, 21\n",
    "\n",
    "target = torch.tensor([3.0, 4.0]) # 3,4\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(25., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "위에서 나온 결과를 바탕으로 \n",
    "경사하강법을 이용해서\n",
    "미분을 이용해서 Loss에 대한 책임을 묻겠다.\n",
    "기울기가 계산이 될 것이다.(alqn)\n",
    "--> BackPropagation을 적용하려면 backword()함수를 호출\n",
    "\n",
    "Loss를 줄여나가 보겠다.\n",
    "'''\n",
    "\n",
    "loss =torch.sum(torch.abs(pred-target))\n",
    "print('Loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.8062, -2.0029, -0.7704],\n",
      "        [-1.2323,  0.3349,  0.0231],\n",
      "        [ 2.0182, -1.5468,  0.5344],\n",
      "        [-1.2959, -0.1772,  0.2497],\n",
      "        [ 0.0675,  0.4059, -0.2344],\n",
      "        [ 1.5122, -0.1740, -0.3413],\n",
      "        [ 0.2838,  0.1838,  0.4825],\n",
      "        [-2.0204, -0.6791,  1.0773],\n",
      "        [ 3.1342,  0.0641, -0.9182],\n",
      "        [-0.6827,  0.8854, -1.4583]])\n",
      "tensor([[ 9.3812e-03,  1.0679e+00],\n",
      "        [ 7.0477e-01, -8.9792e-01],\n",
      "        [ 4.4106e-01, -3.0776e-01],\n",
      "        [ 1.7900e+00,  2.8662e+00],\n",
      "        [-9.9705e-01,  5.6570e-01],\n",
      "        [ 1.3301e+00, -1.5544e-03],\n",
      "        [-7.4095e-01, -3.8730e-02],\n",
      "        [ 5.5188e-01,  5.0212e-02],\n",
      "        [-4.4078e-01,  1.5228e+00],\n",
      "        [-1.2357e+00,  2.1785e+00]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10,3)\n",
    "y = torch.randn(10,2)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w Parameter containing:\n",
      "tensor([[-0.0923, -0.5467, -0.1754],\n",
      "        [-0.1942, -0.0496, -0.4389]], requires_grad=True)\n",
      "b Parameter containing:\n",
      "tensor([-0.4963, -0.4597], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(3,2)\n",
    "print('w',linear.weight)\n",
    "print('b',linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000001D00573BE40>\n",
      "loss before step BackPropagation 1.6448745727539062\n"
     ]
    }
   ],
   "source": [
    "# 객체출력, parameter() 는 모델안의 학습의 주체인w, b를 내포하고 있는 객체\n",
    "# parameter()는 w, b를 해킹한 함수\n",
    "print(linear.parameters())\n",
    "\n",
    "#:loss function을 미리 선정의 해둠\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# optimizer(하산하는 방법)를 미리 정의\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# 위에서 만든 모델에 값을 입력 :: 결과로 예측값이 나옴\n",
    "pred = linear(x)\n",
    "\n",
    "# 정답과 예측값을 이용해서 위에서 선 정의한 loss를 계산\n",
    "loss = loss_function(pred, y)\n",
    "print('loss before step BackPropagation',loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw tensor([[-0.3184, -0.0096, -0.4638],\n",
      "        [-0.4283,  0.2159,  0.7072]])\n",
      "dL/db tensor([-0.9088, -2.1237])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "loss값이 나왔다는 것은 \n",
    "모델의 예측값에 대한 잘잘못을 정량화 했다는 것을 의미.\n",
    "\n",
    "이 값을 바탕으로 BackPropagation을 하면 w, b에 대한 미분 값\n",
    "즉, 얼마만큼 보정해야 하는지의 값이 나옴\n",
    "그 값을 이용해서 다시 하강(기울기 수정)하기 때문에\n",
    "2번째에는 loss가 줄어들어야 한다.\n",
    "'''\n",
    "\n",
    "loss.backward()\n",
    "print('dL/dw',linear.weight.grad)\n",
    "print('dL/db',linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after step BackPropagation 1.6448745727539062\n"
     ]
    }
   ],
   "source": [
    "optimizer.step() # 위에서 수정된 값으로 하강을 진행한다 :: 학습을 ㅏㄴ다.\n",
    "\n",
    "# 반복효과\n",
    "\n",
    "pred = linear(x)\n",
    "loss = loss_function(pred,y)\n",
    "\n",
    "print('loss after step BackPropagation',loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
