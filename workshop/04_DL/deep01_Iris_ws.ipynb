{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLearn Iris Data Loader and DataFrame Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'C:\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\iris.csv'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write Code !!\n",
    "iris = datasets.load_iris()\n",
    "iris\n",
    "\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df\n",
    "\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X,y data Generator...Feature and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Write Code !!\n",
    "# 속성을 X에 할당, 라벨을 y에 각각 할당한다\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Test 데이타를 8:2로 비율로 섞고, random_state=42로 지정\n",
    "    X_train, X_test, y_train, y_test 로 각각 할당된 값들을 torch 타입으로 변환 \n",
    "    torch.FloatTensor(), torch.LongTensor 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Write Code !!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 하이퍼파라미터 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "hidden_size = 56\n",
    "num_classes = 3\n",
    "num_epochs = 5\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuralNetwork  Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    # 모델 설계\n",
    "    def __init__(self, input_size,  hidden_size  ,num_classes):\n",
    "\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1=nn.Linear(input_size, hidden_size)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.fc2=nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    # 모델의 Forward Path를 정의\n",
    "    def forward(self, x):\n",
    "        out=self.fc1(x)        \n",
    "        out=self.relu(out)\n",
    "        out=self.fc2(out)\n",
    "\n",
    "        return out  # 여기까지가 클래스 정의부분\n",
    "    \n",
    "\n",
    "# 위에서 정의한 클래스를 인스턴스화 시킴\n",
    "model = NeuralNet(input_size, hidden_size, num_classes) # 이 model 을 gpu서버에서 돌리겠다\n",
    "\n",
    "# loss and optimizer를 선정의\n",
    "loss_function = nn.CrossEntropyLoss() # Loss 기능안에 Softmax 함수기능 포함되어져 있다.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuralNetwork  Model Excution , loss, optimizer, backward ..\n",
    "    Forward Propagation and Baward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.9732168912887573\n",
      "Epoch: 10 Loss: 0.40592604875564575\n",
      "Epoch: 20 Loss: 0.24042804539203644\n",
      "Epoch: 30 Loss: 0.14590224623680115\n",
      "Epoch: 40 Loss: 0.10282997786998749\n",
      "Epoch: 50 Loss: 0.0841495543718338\n",
      "Epoch: 60 Loss: 0.07478267699480057\n",
      "Epoch: 70 Loss: 0.06939036399126053\n",
      "Epoch: 80 Loss: 0.06595335900783539\n",
      "Epoch: 90 Loss: 0.06356769055128098\n"
     ]
    }
   ],
   "source": [
    "# Write Code !!\n",
    "\n",
    "epochs = 100\n",
    "loss_arr = []\n",
    "for i in range(epochs):\n",
    "    y_hat = model.forward(X_train)\n",
    "    loss = loss_function(y_hat, y_train)\n",
    "    loss_arr.append(loss)\n",
    " \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch: {i} Loss: {loss}')\n",
    "# 학습은 100번을 반복합니다 학습이 진행됨에 따라서 Loss가 감소하는 것을 볼수 있도록 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  학습을 마친 최종적인 모델의 값을 저장. model.ckpt 파일로 저장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch(학습)에 따른 Loss감소를 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write Code !!\n",
    "plt.plot(range(epochs),  loss_arr)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습을 통해서 Loss를 감소시켰다면 이제는Test를 해봅니다.\n",
    "    테스트 할때는 학습의 의미가 없기때문에 Gradient Descent를 사용하지 않도록 합니다.\n",
    "    그 결과로 컴퓨터 Performance를 높이는 결과를 가져옵니다.\n",
    "    이때 우리가 테스트하는 데이타는 이미지가 아니고 단순 숫자 값으로 입력된다는 점을 잘 고려해야합니다.\n",
    "    출력된 값 중에서 가장 높은 값의 인덱스가 바로 target의 라벨이 됩니다.\n",
    "    \n",
    "    예측한 값과 정답을 일일이 비교해서 출력하고\n",
    "    총 30개의 Test 데이타 중에서 정확하게 맞춘 갯수를 최종적으로 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.) 1 1\n",
      "2.) 0 0\n",
      "3.) 2 2\n",
      "4.) 1 1\n",
      "5.) 1 1\n",
      "6.) 0 0\n",
      "7.) 1 1\n",
      "8.) 2 2\n",
      "9.) 1 1\n",
      "10.) 1 1\n",
      "11.) 2 2\n",
      "12.) 0 0\n",
      "13.) 0 0\n",
      "14.) 0 0\n",
      "15.) 0 0\n",
      "16.) 1 1\n",
      "17.) 2 2\n",
      "18.) 1 1\n",
      "19.) 1 1\n",
      "20.) 2 2\n",
      "21.) 0 0\n",
      "22.) 2 2\n",
      "23.) 0 0\n",
      "24.) 2 2\n",
      "25.) 2 2\n",
      "26.) 2 2\n",
      "27.) 2 2\n",
      "28.) 2 2\n",
      "29.) 0 0\n",
      "30.) 0 0\n",
      "30개 중 30개 맞춤!\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "with torch.no_grad(): \n",
    "   \n",
    "    # Write Code!!\n",
    "    for i, data in enumerate(X_test):\n",
    "        y_val = model.forward(data)\n",
    "        \n",
    "        print(f'{i+1}.) {str(y_val.argmax().item())} {y_test[i]}')\n",
    "        \n",
    "        if y_val.argmax().item() == y_test[i]:\n",
    "            correct += 1\n",
    "            \n",
    "print(f'30개 중 {correct}개 맞춤!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
